{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caIlCAmQvukt"
   },
   "source": [
    "# CSE 152A Fall 2022 – Assignment 4Fri\n",
    "\n",
    "Assignment Published On: **Fri, Nov 18, 2022**\n",
    "\n",
    "Due On: **Thur, Dec 1, 2022 11:59 PM (Pacific Time)**\n",
    "\n",
    "Instructions:\n",
    "- Attempt all questions.\n",
    "- Please comment all your code adequately.\n",
    "- Please write your code at the ``WRITE YOUR CODE HERE'' prompt in the .ipynb file. \n",
    "\n",
    "## Instructions for Google Colab\n",
    "You will be running this homework on google colab which provides free GPU resources which we can use for accelerating deep learning. Google colab is an online jupyter notebook which can run code on GPU. \n",
    "\n",
    "To get started:\n",
    "\n",
    "- Navigate to https://colab.research.google.com\n",
    "If asked for login, sign in with any google account (your ucsd id works too).\n",
    "- Upload the HW4.ipynb to colab. You can do this by `File -> upload notebook`\n",
    "- Go to `Runtime -> Change run time type -> Select GPU from the dropdown`\n",
    "- Connect to the instance\n",
    "- All required packages are already installed in the environment. In case you would like to install a package you can simply run `!pip install <package-name>`.\n",
    "\n",
    "\n",
    "### Note:\n",
    "Since you will be using the free tier, you can run a notebook for a maximum of 12hrs at a stretch after which you need to reload the notebook. \n",
    "We suggest you switch to GPU instance when you need to train or test the model and use the CPU instance for running non-mathematical operations. You can switch instances between the CPU and GPU anytime using the `Runtime` menu without loss of work.\n",
    "\n",
    "In the rare chance you don't get a GPU instance try again in 15mins and use the CPU instance until then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIQ-nF1le1IS"
   },
   "source": [
    "# Theory Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si4KWIjsX_Kq"
   },
   "source": [
    "## 1. Backpropogation\n",
    "\n",
    "Please refer to \"q1.png\" for questions and write your answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3EnK7-Ac1uX"
   },
   "source": [
    "## 1. (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J0h0stSc6YU"
   },
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxWqOHdBc8Qe"
   },
   "source": [
    "## 1. (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuEjK8LSdAly"
   },
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCFEKgdrdCXJ"
   },
   "source": [
    "## 1. (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ct-E7YlndDey"
   },
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty86EmxbdD1a"
   },
   "source": [
    "## 1. (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVwqOkkAdE6v"
   },
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpSxInJ4dN6x"
   },
   "source": [
    "# Programming Problem\n",
    "\n",
    "**You need to have an enviroment with CUDA GPU to finish this homework**. If you don't have access to such environment, a good platform to look at is Google Colab, which provides a free CUDA GPU server.\n",
    "\n",
    "**You also need to download MNIST and CIFAR-10 dataset to accomplish this homework.** The following cell is given to help you download them. It requires certain tool to be installed in your environment. You can either use this cell to download the datasets or manually download them on the corresponding websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxIzvspSPf5v"
   },
   "outputs": [],
   "source": [
    "# Get necessary datasets; this cell only needs to be run once\n",
    "!if [ -e \"MNIST\" ];then rm -rf \"MNIST\" ; fi\n",
    "!if [ -e \"CIFAR-10\" ];then rm -rf \"CIFAR-10\" ; fi\n",
    "\n",
    "!mkdir MNIST\n",
    "%cd MNIST\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "!wget http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "!gzip -d train-images-idx3-ubyte.gz\n",
    "!gzip -d train-labels-idx1-ubyte.gz\n",
    "!gzip -d t10k-images-idx3-ubyte.gz\n",
    "!gzip -d t10k-labels-idx1-ubyte.gz\n",
    "%cd ..\n",
    "\n",
    "!mkdir CIFAR-10\n",
    "%cd CIFAR-10\n",
    "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "!tar -xvzf cifar-10-python.tar.gz\n",
    "!rm cifar-10-python.tar.gz\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "proTjsMjX_Kr"
   },
   "source": [
    "## 2. Training a small CNN for MNIST digit classification\n",
    "\n",
    "In this problem, you will train a small convolutional neural network for image classification, using PyTorch. We will use the MNIST dataset for digit classification (http://yann.lecun.com/exdb/mnist/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4vY-bSuX_Kr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import struct\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EJO3niHX_Kr"
   },
   "outputs": [],
   "source": [
    "# DATA PARSING\n",
    "# You can manually download the data from http://yann.lecun.com/exdb/mnist/ and set path\n",
    "path = \"./MNIST/\"\n",
    "\n",
    "def read(dataset = \"training\", datatype='images'):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "    \n",
    "    if(datatype=='images'):\n",
    "        get_data = lambda idx: img[idx]\n",
    "    elif(datatype=='labels'):\n",
    "        get_data = lambda idx: lbl[idx]\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_data(i)\n",
    "        \n",
    "trainData=np.array(list(read('training','images')))\n",
    "trainData=np.float32(np.expand_dims(trainData,-1))/255\n",
    "torchTrainData=trainData.transpose((0,3,1,2))\n",
    "trainLabels=np.int32(np.array(list(read('training','labels'))))\n",
    "\n",
    "testData=np.array(list(read('testing','images')))\n",
    "testData=np.float32(np.expand_dims(testData,-1))/255\n",
    "torchTestData=testData.transpose((0,3,1,2))\n",
    "testLabels=np.int32(np.array(list(read('testing','labels'))))\n",
    "\n",
    "plt.figure()\n",
    "for i in range(10):\n",
    "    ind=np.where(trainLabels==i)[0][0]\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(trainData[ind][:,:,0],cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMlAUsdUX_Ks"
   },
   "source": [
    "**[ 3 points ] Define the network structure as follows**\n",
    "\n",
    "* Convolutional layer with 32 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Convolutional layer with 64 kernels, window size 5, padding size 2, stride 1\n",
    "* In place ReLU activation layer\n",
    "* Max pooling layer with window size 2, stride 2\n",
    "* Fully connected layer with 1024 output channels\n",
    "* In place ReLU activation layer\n",
    "* Dropout layer with drop rate 0.4\n",
    "* Fully connected layer with 10 output channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGROQGfcX_Ks"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,drop):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # DEFINE THE NETWORK STRUCTURE\n",
    "        \n",
    "        # Example: self.conv1 = nn.Conv2d(1, 3, 5,stride=1,padding=2,bias=True)\n",
    "        # You can look at https://github.com/ameykusurkar/pytorch-image-classifier/blob/master/main.py for reference\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Example: x = self.conv1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Print net\n",
    "net = Net(drop=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1p7pVx6X_Ks"
   },
   "source": [
    "**[ 5 points ] Complete the train function below. Use the same parameters to perform training in each of the following setups:**\n",
    "* SGD for optimization, without dropout\n",
    "* SGD for optimization, with dropout\n",
    "* Adam for optimization, without dropout\n",
    "* Adam for optimization, with dropout.\n",
    "\n",
    "As evaluation for each case above, perform the following:\n",
    "* Plot the loss graph and the accuracy graph on training set on the same plot\n",
    "* Print the accuracy on test set\n",
    "\n",
    "Test accuracies are expected to be quite high (~98 %)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz6nyih9X_Kt"
   },
   "outputs": [],
   "source": [
    "# CODE BELOW IS AN EXAMPLE STARTER\n",
    "# FEEL FREE TO EDIT ANYTHING\n",
    "\n",
    "# to_train is a parameter that determines what part of the net to train\n",
    "# it is not required for this question, but will be useful in the next one\n",
    "def train(tdata,tlabel,net,to_train,opt):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist=[]\n",
    "    \n",
    "    # Change parameters as required\n",
    "    epochs=15\n",
    "    batch=200\n",
    "    learning_rate=1e-3\n",
    "    \n",
    "    if(opt=='adam'):\n",
    "        optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.SGD(to_train,lr=learning_rate,momentum = 0.99)\n",
    "        \n",
    "    for k in tqdm(range(epochs)):\n",
    "        for l in range(int(len(tdata)/batch)):\n",
    "            inds=np.random.randint(0,len(tdata)-1,batch)\n",
    "            inputs = Variable(torch.FloatTensor(tdata[inds]).cuda())\n",
    "            targets = Variable(torch.LongTensor(tlabel[inds]).cuda())\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            # Train the model using the optimizer and the batch data\n",
    "            # Append the loss and accuracy to the losslist and acclist arrays\n",
    "\n",
    "    return losslist,acclist\n",
    "\n",
    "def test(tdata,tlabel,net):\n",
    "    inputs = Variable(torch.FloatTensor(tdata).cuda())\n",
    "    targets = Variable(torch.LongTensor(tlabel).cuda())\n",
    "    prediction = net(inputs)\n",
    "    acc=np.mean(np.argmax(prediction.data.cpu().numpy(),1)==tlabel)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M8C0umBX_Kt"
   },
   "outputs": [],
   "source": [
    "# Example code \n",
    "\n",
    "net = Net(drop=False).cuda()\n",
    "loss,acc=train(torchTrainData,trainLabels,net,net.parameters(),'sgd')\n",
    "ax=range(len(x1))\n",
    "plt.plot(ax,loss,ax,acc)\n",
    "plt.legend(['loss','accuracy'])\n",
    "plt.show()\n",
    "print('Accuracy:{}'.format(test(torchTestData,testLabels,net)))\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKNOd80VX_Kt"
   },
   "source": [
    "**[ 5 points ] Plot the following graphs and comment on them**\n",
    "\n",
    "* Training loss graphs of SGD−dropout and Adam−dropout on the same plot \n",
    "* Training loss graphs for Adam-dropout for 3 different values of batch size such that there is some difference in the graphs, on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8a-zOi8XX_Ky"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjFXYu_FX_Kz"
   },
   "source": [
    "**[ 2 points ] Plot the training loss graphs for changes made in any particular parameter (learning rate/momentum etc) such that there is a clear difference in the graphs, on the same plot, and comment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovuF7sliX_Kz"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvyEGuJzX_Kz"
   },
   "outputs": [],
   "source": [
    "###############################  QUESTION 3  ##############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U3G8vllX_Kz"
   },
   "source": [
    "## 3. Transfer learning\n",
    "\n",
    "You will now visualize the effects of transfer learning by performing experiments using the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html) . Note that this is just to understand how transfer learning works, in practice it is generally used with very large datasets and complex networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQ4LFINCX_Kz"
   },
   "outputs": [],
   "source": [
    "# DATA PARSING\n",
    "# You can manually download the data from from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and set path\n",
    "path='./CIFAR-10/cifar-10-batches-py/'\n",
    "data=np.zeros((0,32,32,3))\n",
    "labels=[]\n",
    "for i in range(1,6):\n",
    "    with open(path+'data_batch_'+str(i), 'rb') as fo:\n",
    "        dat = pickle.load(fo,encoding='latin1')\n",
    "        r=dat['data'][:,:1024*1].reshape((10000,32,32,1))\n",
    "        g=dat['data'][:,1024:2048].reshape((10000,32,32,1))\n",
    "        b=dat['data'][:,2048:3072].reshape((10000,32,32,1))\n",
    "        rgb=np.concatenate((r,g,b),axis=3)\n",
    "        data=np.vstack((data,np.float32(rgb)/255))\n",
    "        labels+=dat['labels']\n",
    "labels=np.array(labels)\n",
    "# data -> 50000 X 32 X 32 X 3 array with training data\n",
    "# labels -> 50000 labels ranging from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rys8siggX_Kz"
   },
   "source": [
    " **[ 2 points ] Plot 3 random images corresponding to each label from the training data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ta9y8-7X_Kz"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20AKWkroX_Kz"
   },
   "outputs": [],
   "source": [
    "# Split the data and labels into 2 sets, first one containing labels 0 to 4, and second one from 5 to 9. \n",
    "\n",
    "data1=np.zeros((0,32,32,3))\n",
    "labels1=[]\n",
    "data2=np.zeros((0,32,32,3))\n",
    "labels2=[]\n",
    "for i in range(5):\n",
    "    x=data[labels==i]\n",
    "    data1=np.vstack((data1,x))\n",
    "    labels1+=[i]*len(x)\n",
    "for i in range(5,10):\n",
    "    x=data[labels==i]\n",
    "    data2=np.vstack((data2,x))\n",
    "    labels2+=[i-5]*len(x)\n",
    "    \n",
    "labels1=np.array(labels1)\n",
    "labels2=np.array(labels2)\n",
    "\n",
    "torch_data1=data1.transpose((0,3,1,2))\n",
    "torch_data2=data2.transpose((0,3,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wuvNYsfX_K0"
   },
   "source": [
    "**[ 3 points ] Create a simple convolutional network to classify the training data. The network structure should be as follows:**\n",
    "1. Layer 1 - Kernel size 4, Stride 2, Output channels 5, Bias enabled, Relu activation\n",
    "2. Layer 2 - Kernel size 4, Stride 1, Output channels 10, Bias enabled, Relu avtication\n",
    "3. Layer 3 - Kernel size 4, Stride 1, Output channels 20, Bias enabled, Relu activation\n",
    "4. Layer 4 - Kernel size 4, Stride 1, Output channels 40, Bias enabled, Relu activation\n",
    "5. Layer 5 - Fully connected layer with 5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3D7uVxxX_K0"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBDn1bxIX_K0"
   },
   "source": [
    "**[ 5 points ] Complete the train function below and follow the instructions** \n",
    "\n",
    "* Initialize the network, train the complete network (net.parameters) on data1 (The first 5 classes)\n",
    "* Plot the loss and accuracy graphs over training on the same plot\n",
    "* Print the final training accuracy as well**\n",
    "\n",
    "Set the learning rate, number of iterations and batch size such that the loss is gradually and smoothly decreasing and converging. The accuracy at the end of training must be around or greater than 60 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3vguCEdX_K0"
   },
   "outputs": [],
   "source": [
    "# to_train can be net.paramaters OR net.fc.parameters OR net.conv1.parameters so that only certain parts of the net are trained\n",
    "def train(tdata,tlabel,net,to_train):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losslist = []\n",
    "    acclist=[]\n",
    "    \n",
    "    # Change as required\n",
    "    epochs=5\n",
    "    batch=100\n",
    "    learning_rate=1e-3\n",
    "    optimizer = optim.Adam(to_train,lr=learning_rate)\n",
    "    \n",
    "    for k in tqdm(range(epochs)):\n",
    "        for l in range(int(len(tdata)/batch)):\n",
    "            inds=np.random.randint(0,len(tdata)-1,batch)\n",
    "            inputs = Variable(torch.FloatTensor(tdata[inds]).cuda())\n",
    "            targets = Variable(torch.LongTensor(tlabel[inds]).cuda())\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "\n",
    "    return losslist,acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ofSpcHfX_K0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nrueess3X_K0"
   },
   "source": [
    "**[ 2 points ] Without reinitializing the network, train only the fully connected layer (net.fc.parameters) now on data2 (The next 5 classes)** \n",
    "\n",
    "Do not change any hyper parameters such as learning rate or batch size. Plot the loss and accuracy and print the final values like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOCyIQxrX_K0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwRz5feqX_K0"
   },
   "source": [
    "**[ 3 points ] Now repeat the process in the opposite order** \n",
    "\n",
    "* Initialize the net again, train the whole network on data2, generate the same plots as before\n",
    "* Then without reinitializing the net, train only the fully connected layer on data1 and generate the plots\n",
    "\n",
    "Do not change any hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qw2M-P7bX_K1"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmd6G4q0X_K1"
   },
   "source": [
    "**[ 5 points ]**\n",
    "\n",
    "* Plot the loss vs iterations for the classifers trained to classify data1, via normal learning as well as transfer learning, on the same plot\n",
    "* Plot another graph for the classifiers trained to classify data2\n",
    "\n",
    "Explain the results obtained, based on the training regimen. Comment on why transfer learning worked/didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JjQ2cXcX_K1"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gk_5Z8VX_K1"
   },
   "source": [
    "Create a network with more layers, pooling layers, and more filters and try to increase accuracy as much as possible. Play around with the hyperparameters to understand how they affect the training process. No need to turn in anything for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9QNA1dxgqWf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
